{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 5: Sound-types\n",
    "\n",
    "As we saw in class, sound-types is a multi-layer framework for representing and synthesizing sounds. In this lab, we will apply the theory of sound-types for creating new audio from input audio. The two main types of synthesis that are possible through the sound-types are *probabilistic generation* and *sound hybridization*. Sections 1 and 2 will walk you through using the sound-types to apply probabilistic generation to an input sound. In Section 3, you will combine two input sounds through the process of sound hybridization.\n",
    "\n",
    "First, let's briefly review the theory of sound-types. The analysis phase of sound-types takes in an input sample as audio, and performs the following steps:\n",
    "1. **atomize**: the input sound is divided into very small (40 ms) overlapping chunks called *atoms*\n",
    "2. **make classes**: for each atom, compute low-level descriptors that allow you to represent the atoms in a feature space. Then, we can use this space to see whcih atoms are closer to each other (and therefore more similar), and creater groups (clusters) of similar atoms.\n",
    "3. **compute probabilities**: finally, determine the sequential relationship between the clusters of atoms computed in the step 2. Using a Markov chain, we can estimate the probabilities that one cluster of atoms is followed by another in the input sound\n",
    "\n",
    "For further details, see: Cella, Carmine-Emanuele & Burred, Juan José. (2013). *Advanced sound hybridizations by means of the theory of sound-types.* ICMC, 2013.\n",
    "\n",
    "## Section 1: Probabilistic Generation\n",
    "\n",
    "So now that we have seen how the analysis phase of the sound-types works, we will use the Markov chain generated in step 3 to create new audio based on our input signal. The Markov chain is a series of states, where each state is a cluster of atoms. We have already computed the probabilities of transitioning between states. Therefore, we can create a new sound is a similar way to how we created Bach chorales in lab 2:\n",
    "\n",
    "1. Randomly select a starting state\n",
    "2. Select an atom from that state\n",
    "3. Using the transition probabilities of that state, select the next state\n",
    "4. Repeat steps 2 and 3 until a stopping condition is met\n",
    "\n",
    "The atoms that we select in step 2 become our generated audio.\n",
    "\n",
    "The following code cells will walk you through running the analysis and synthesis portions of the sound-types. Run each cell in order and listen to the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from st_tools import make_soundtypes\n",
    "from IPython.display import Audio, display\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "\n",
    "N_COEFF = 14\n",
    "ST_RATIO = .9\n",
    "N_FRAMES = 500\n",
    "FRAME_SIZE = 1024\n",
    "HOP_SIZE = 512\n",
    "MAX_LOOPS = 3 \n",
    "SR = 44100\n",
    "\n",
    "SAMPLES_PATH = Path('./samples')\n",
    "\n",
    "def plot_audio(audio, sr):\n",
    "    plt.figure(tight_layout=True)\n",
    "\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    ax1.axis('off')\n",
    "    plt.title('Waveform')\n",
    "    librosa.display.waveshow(audio, sr=sr)\n",
    "\n",
    "    ax2 = plt.subplot(2,1,2, sharex=ax1)\n",
    "    plt.title('Spectrogram')\n",
    "    stft = librosa.stft(audio)\n",
    "    stft_db = librosa.amplitude_to_db(np.abs(stft))\n",
    "    librosa.display.specshow(stft_db, sr=sr, x_axis='time', y_axis='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an input sound file from the dropdown below. Then run the next cell to listen to and visualize the sound. When you change to a new sound file, you need to rerun the second cell as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples = list(SAMPLES_PATH.iterdir())\n",
    "samples = [file.name for file in samples]\n",
    "w = widgets.Dropdown(options=samples, value='bass.wav', description='Select file:')\n",
    "input_file = SAMPLES_PATH / w.value\n",
    "y = None\n",
    "sr = None\n",
    "\n",
    "y, sr = librosa.load(input_file, sr=SR)\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global input_file\n",
    "        global y\n",
    "        global sr\n",
    "        \n",
    "        input_file = SAMPLES_PATH / change.new\n",
    "        y, sr = librosa.load(input_file, sr=SR)\n",
    "        print(\"changed sample to\", input_file)\n",
    "        \n",
    "w.observe(on_change)\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input audio:\", input_file)\n",
    "\n",
    "plot_audio(y, sr)\n",
    "\n",
    "Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('[soundtypes - probabilistic generation]\\n')\n",
    "print ('computing features...')\n",
    "y_pad = np.zeros(len(y) + FRAME_SIZE)\n",
    "y_pad[1:len(y)+1] = y\n",
    "C = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_COEFF, n_fft=FRAME_SIZE, \n",
    "                         hop_length=HOP_SIZE)\n",
    "\n",
    "print ('multidimensional scaling...')\n",
    "mds = MDS(2)\n",
    "C_scaled = mds.fit_transform (C.T)\n",
    "\n",
    "print ('computing soundtypes...')\n",
    "(dictionary, markov, centroids, labels) = \\\n",
    "    make_soundtypes(C_scaled, ST_RATIO)\n",
    "n_clusters = centroids.shape[0]\n",
    "\n",
    "# print (markov)\n",
    "print ('generate new sequence...')\n",
    "w1 = np.random.randint (n_clusters)\n",
    "prev_w1 = 0\n",
    "loops = 0\n",
    "gen_sequence = []\n",
    "gen_sound = np.zeros(N_FRAMES * HOP_SIZE + FRAME_SIZE)\n",
    "for i in range(N_FRAMES):\n",
    "    l = markov[(w1)]\n",
    "    if len(l) == 0:\n",
    "        w1 = np.random.randint(n_clusters)\n",
    "    else:\n",
    "        w1 = l[np.random.randint(len(l))]\n",
    "    if prev_w1 == w1:\n",
    "        loops += 1\n",
    "\n",
    "    if loops > MAX_LOOPS:\n",
    "        w1 = np.random.randint(n_clusters)\n",
    "        loops = 0\n",
    "\n",
    "    gen_sequence.append(w1)\n",
    "    p = dictionary[(w1)]\n",
    "    atom = p[np.random.randint(len(p))]\n",
    "\n",
    "    chunk = y_pad[atom*HOP_SIZE:atom*HOP_SIZE+FRAME_SIZE] \\\n",
    "        * np.hanning(FRAME_SIZE)\n",
    "    gen_sound[i*HOP_SIZE:i*HOP_SIZE+FRAME_SIZE] += chunk\n",
    "\n",
    "print ('saving audio data...')\n",
    "sf.write('generated_sound.wav', gen_sound, sr)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can listen to the generated sound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Generated audio:\")\n",
    "plot_audio(gen_sound, sr)\n",
    "Audio(gen_sound, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Probabilistic generation with onsets\n",
    "\n",
    "In this section we again perform probabilistic generation, however the way that we create the atoms is different. In part 1, every atom was of equal length, around 40 ms. Now, we will try to compute when a new *onset* occurs in the input sound, and have each onset be a new atom. For example, if the input is a recording of a piano playing a melody, each new note in the melody is a new onset. The onsets are determined by measuring the *spectral flux*, which measures how quickly the spectrum of the sound is changing. When there is a new onset, for example a note played on the piano, the spectral flux has a high value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from st_tools import get_segments, make_soundtypes\n",
    "\n",
    "N_COEFF = 20\n",
    "ST_RATIO = .5\n",
    "N_FRAMES = 100\n",
    "FRAME_SIZE = 1024\n",
    "HOP_SIZE = 1024\n",
    "MAX_LOOPS = 3\n",
    "WIDTH = 16\n",
    "FADE_MS = 10\n",
    "SR = 44100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the previous part, we first set our input sound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(SAMPLES_PATH.iterdir())\n",
    "samples = [file.name for file in samples]\n",
    "w = widgets.Dropdown(options=samples, value='Jarrett_Vienna_cut.wav', description='Select file:')\n",
    "input_file = SAMPLES_PATH / w.value\n",
    "y = None\n",
    "sr = None\n",
    "\n",
    "y, sr = librosa.load(input_file, sr=SR)\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global input_file\n",
    "        global y\n",
    "        global sr\n",
    "        \n",
    "        input_file = SAMPLES_PATH / change.new\n",
    "        y, sr = librosa.load(input_file, sr=SR)\n",
    "        print(\"changed sample to\", input_file)\n",
    "        \n",
    "w.observe(on_change)\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Input audio:\", input_file)\n",
    "\n",
    "plot_audio(y, sr)\n",
    "\n",
    "Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print ('[soundtypes - probabilistic generation on onsets]\\n')\n",
    "print ('computing segments...')\n",
    "\n",
    "\n",
    "(segments, onsets, flux) = get_segments (y, sr, FRAME_SIZE, HOP_SIZE, \\\n",
    "    FADE_MS, WIDTH)\n",
    "\n",
    "print ('computing features...')\n",
    "features = []\n",
    "for i in range (len(segments)):\n",
    "    C = librosa.feature.mfcc(y=segments[i], sr=sr, n_mfcc=N_COEFF,\n",
    "                             n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
    "    features.append(np.mean (C, axis=1))\n",
    "\n",
    "C = np.vstack(features)\n",
    "\n",
    "print ('multidimensional scaling...')\n",
    "mds = MDS(2)\n",
    "C_scaled = mds.fit_transform (C)\n",
    "\n",
    "print ('computing soundtypes...')\n",
    "(dictionary, markov, centroids, labels) = \\\n",
    "    make_soundtypes(C_scaled, ST_RATIO)\n",
    "n_clusters = centroids.shape[0]\n",
    "\n",
    "print ('generate new sequence...')\n",
    "w1 = np.random.randint (n_clusters)\n",
    "prev_w1 = 0\n",
    "loops = 0\n",
    "gen_sequence = []\n",
    "gen_sound = []\n",
    "for i in range(N_FRAMES):\n",
    "    l = markov[(w1)]\n",
    "    if len(l) == 0:\n",
    "        w1 = np.random.randint(n_clusters)\n",
    "    else:\n",
    "        w1 = l[np.random.randint(len(l))]\n",
    "    if prev_w1 == w1:\n",
    "        loops += 1\n",
    "\n",
    "    if loops > MAX_LOOPS:\n",
    "        w1 = np.random.randint(n_clusters)\n",
    "        loops = 0\n",
    "\n",
    "    gen_sequence.append(w1)\n",
    "    p = dictionary[(w1)]\n",
    "    atom = p[np.random.randint(len(p))]\n",
    "\n",
    "    gen_sound.append (segments[atom])\n",
    "\n",
    "gen_sound = np.hstack (gen_sound)\n",
    "\n",
    "print ('saving audio data...')\n",
    "sf.write('generated_sound.wav', gen_sound, sr)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can listen to the generated sound. Compare this generation to the output from part 1. Are there differences? Does one work \"better\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Generated audio:\")\n",
    "plot_audio(gen_sound, sr)\n",
    "Audio(gen_sound, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sound Hybridization\n",
    "\n",
    "Another operation possible through the sound-types is sound hybridization. Similar to style transfer, sound hybridization takes the spectral content of one sound and applies it to the temporal aspects of another sound.\n",
    "\n",
    "Here is a description of the process from the paper on sound-types:\n",
    "\n",
    "\"It is possible to subject two different sounds to separate types and rules inferences, and then impose or merge one sound’s types or rules with the others’. The sound-types inferred from a signal (the source) are replaced by, or merged with, the sound-types inferred from a target signal. Each sound-type from the source is matched with a sound-type from the target, in terms of a similarity measure between the centroids of their corresponding feature clusters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from st_tools import make_soundtypes\n",
    "import soundfile as sf\n",
    "\n",
    "N_COEFF = 13\n",
    "FRAME_SIZE = 2048\n",
    "HOP_SIZE = 1024\n",
    "ST_RATIO = .7\n",
    "K = 5\n",
    "SR = 44100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, set the \"source\" and \"target\" files, then running the cell after to visualize and listen to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Source File\n",
    "\n",
    "samples = list(SAMPLES_PATH.iterdir())\n",
    "samples = [file.name for file in samples]\n",
    "w_src = widgets.Dropdown(options=samples, value='cage.wav', description='Source File:')\n",
    "source_file = SAMPLES_PATH / w_src.value\n",
    "y_src = None\n",
    "sr = None\n",
    "\n",
    "y_src, sr = librosa.load(source_file, sr=SR)\n",
    "\n",
    "def on_change_src(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global source_file\n",
    "        global y_src\n",
    "        global sr\n",
    "        \n",
    "        source_file = SAMPLES_PATH / change.new\n",
    "        y_src, sr = librosa.load(source_file, sr=SR)\n",
    "        print(\"changed source file to\", source_file)\n",
    "        \n",
    "w_src.observe(on_change_src)\n",
    "\n",
    "# Target File\n",
    "\n",
    "w_dst = widgets.Dropdown(options=samples, value='lachenmann.wav', description='Target File:')\n",
    "target_file = SAMPLES_PATH / w_dst.value\n",
    "y_dst = None\n",
    "sr = None\n",
    "\n",
    "y_dst, sr = librosa.load(target_file, sr=SR)\n",
    "\n",
    "def on_change_dst(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global target_file\n",
    "        global y_dst\n",
    "        global sr\n",
    "        \n",
    "        target_file = SAMPLES_PATH / change.new\n",
    "        y_dst, sr = librosa.load(target_file, sr=SR)\n",
    "        print(\"changed destination file to\", target_file)\n",
    "        \n",
    "w_dst.observe(on_change_dst)\n",
    "\n",
    "display(w_src, w_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source audio:\", source_file)\n",
    "plot_audio(y_src, sr)\n",
    "display(Audio(y_src, rate=sr))\n",
    "\n",
    "print(\"Target audio:\", target_file)\n",
    "plot_audio(y_dst, sr)\n",
    "display(Audio(y_dst, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting your source and target files, run the following code to generate a new sample that is a hybridization of the two sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print ('[soundtypes - timbre matching]\\n')\n",
    "print ('computing features...')\n",
    "\n",
    "y_pad_src = np.zeros(len(y_src) + FRAME_SIZE)\n",
    "y_pad_src[1:len(y_src)+1] = y_src\n",
    "\n",
    "C_src = librosa.feature.mfcc(y=y_src, sr=sr, n_mfcc=N_COEFF,\n",
    "                                 n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
    "\n",
    "y_pad_dst = np.zeros(len(y_dst) + FRAME_SIZE)\n",
    "y_pad_dst[1:len(y_dst)+1] = y_dst\n",
    "\n",
    "C_dst = librosa.feature.mfcc(y=y_dst, sr=sr, n_mfcc=N_COEFF,\n",
    "                                 n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
    "\n",
    "C_scaled_dst = C_dst.T\n",
    "C_scaled_src = C_src.T\n",
    "\n",
    "scaler = StandardScaler ()\n",
    "C_scaled_dst = scaler.fit_transform (C_scaled_dst)\n",
    "C_scaled_src = scaler.fit_transform (C_scaled_src)\n",
    "\n",
    "print ('computing soundtypes...')\n",
    "(dictionary_src, markov_src, centroids_src, labels_src) = \\\n",
    "    make_soundtypes(C_scaled_src, ST_RATIO)\n",
    "n_clusters_src = centroids_src.shape[0]\n",
    "(dictionary_dst, markov_dst, centroids_dst, labels_dst) = \\\n",
    "    make_soundtypes(C_scaled_dst, ST_RATIO)\n",
    "n_clusters_dst = centroids_dst.shape[0]\n",
    "\n",
    "print ('matching clusters...')\n",
    "knn = NearestNeighbors(n_neighbors=K).fit(centroids_dst)\n",
    "dist, idxs = knn.kneighbors(centroids_src)\n",
    "\n",
    "print ('generate hybridization...')\n",
    "n_frames = len(labels_src)\n",
    "gen_sound = np.zeros(n_frames * HOP_SIZE + FRAME_SIZE)\n",
    "for i in range(n_frames):\n",
    "    labels_match = idxs[labels_src[i], :]\n",
    "    x = labels_match[np.random.randint(K)]\n",
    "    p = dictionary_dst[x]\n",
    "    if len(p) == 0:\n",
    "        atom = 0\n",
    "    else:\n",
    "        atom = p[np.random.randint(len(p))]\n",
    "\n",
    "    amp = np.sum (np.abs(y_pad_src[i * HOP_SIZE : i * HOP_SIZE + \\\n",
    "        FRAME_SIZE]))\n",
    "    chunk = y_pad_dst[atom * HOP_SIZE : atom * HOP_SIZE + FRAME_SIZE] \\\n",
    "        * np.hanning(FRAME_SIZE)\n",
    "\n",
    "    norm = np.max (np.abs(chunk))\n",
    "    if norm == 0:\n",
    "        norm = 1\n",
    "\n",
    "    chunk /= norm\n",
    "    gen_sound[i * HOP_SIZE : i * HOP_SIZE + FRAME_SIZE] += (chunk * amp / n_frames)\n",
    "\n",
    "print ('saving audio data...')\n",
    "sf.write('generated_sound.wav', gen_sound, sr)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, listen to the generated sound by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Generated audio:\")\n",
    "plot_audio(gen_sound, sr)\n",
    "Audio(gen_sound, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about this sound hybridization process? Do you consider it successful at style transfer? Why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
