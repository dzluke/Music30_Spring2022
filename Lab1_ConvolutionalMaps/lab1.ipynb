{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-lab: How to use a Jupyter Notebook\n",
    "\n",
    "Welcome to Lab 1! What you're looking at right now is a Jupyter Notebook, hosted on GitHub, running on Berkeley's Datahub service. What this means is that all the code in this file is hosted remotely, not on your computer. Therefore, you don't need to perform any setup on your computer to get this running! \n",
    "\n",
    "A notebook is a series of cells. Each cell can contain text (like this one) or code (like the one below). Any cell can be edited by double-clicking on it. A cell is run by pressing `shift` + `return`. Running a text cell exits the editing mode. Running a code cell runs the code and displays the output below the cell.\n",
    "\n",
    "For this class, you will not need to edit any text or code cells (although you are welcome to if you'd like!). In lab, you will only need to run the cells we have created for you.\n",
    "\n",
    "To make sure you know how to run cells, click on the code cell below and press `shift` + `return`. After running the cell, you should see the text `Hello, World!` underneath the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try running this next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\N{VIOLIN} \\N{MULTIPLE MUSICAL NOTES} \\N{TRUMPET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see three musical emojis, then things are going well for you. **The following lab consists of code and text cells. Run each code cell (in the order presented) and read each text cell.** You don't need to run the text cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Convolutional Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(note: you won't see any output beneth the following code cell, but it's still important to run)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage import io, color\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from scipy import signal\n",
    "from ipywidgets import interact \n",
    "\n",
    "from IPython.display import Audio, display\n",
    "import soundfile as sf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blurring an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore blurring images using a kernel. A **kernel** is a matrix that can be used for image processing, such as blurring. To blur an image using a kernel we will perform a convolution between a kernel and an image. A **convolution** is a mathematical operation on two functions (in this case our two matrices) that produces a third function (our blurred image).\n",
    "\n",
    "A **matrix** is a rectangular array of numbers arranged in horizontal rows and vertical columns. Below is an example of a 4 by 4 matrix where each value in the matrix is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.ones((4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import an image of a butterfly that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "butterfly = io.imread('assets/butterfly.jpg');\n",
    "\n",
    "#convert to gray scale color map\n",
    "gray_butterfly = color.rgb2gray(butterfly)\n",
    "\n",
    "# Show image\n",
    "plt.figure(); \n",
    "io.imshow(gray_butterfly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a random 5 by 5 matrix that we can use as our kernel to blur the butterfly image. Try running the cell below a few times and you will see that each time it outputs a different randomly produced matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "random_matrix = np.random.rand(5,5)\n",
    "random_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convolve the image with the random matrix. Notice the difference between this and the original image. By convolving our image with a matrix of random values, we have blurred the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#using signal.convolve2d to blur butterfly\n",
    "blurred_butterfly = signal.convolve2d(gray_butterfly, random_matrix)\n",
    "\n",
    "#show blurred image\n",
    "io.imshow(blurred_butterfly, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we used a larger or smaller size matrix to blur the image? Move the slider below find out. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def SquareKernel(matrix_size):\n",
    "    return np.random.rand(matrix_size, matrix_size)\n",
    "\n",
    "def AdjustKernelRadius(matrix_size):\n",
    "    fim = signal.convolve2d(gray_butterfly, SquareKernel(matrix_size))\n",
    "    io.imshow(fim, cmap='gray')\n",
    "interact(AdjustKernelRadius, matrix_size=(1,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Edge Detection \n",
    "\n",
    "Now we will attempt to use convolution to detect the edges in an image.\n",
    "\n",
    "First, let's take a look at our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "image = plt.imread('assets/mozart.png')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see the dimensions of our matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"The dimensions of our matrix are: \", image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix has the shape 256 by 256 by 3. This means the image is 256 pixels by 256 pixels, and each pixel is represented by 3 values: the red, green, and blue values.\n",
    "\n",
    "As we saw in the previous example, a grayscale image is stored as a two-dimensional matrix, where each entry in the matrix corresponds to the intensity of a single pixel of the image. A color image is stored as a three-dimensional matrix, since each pixel requires not just one intensity value, but three: one for red, one for green, and one for blue.\n",
    "\n",
    "However, our input matrix must be two dimensional in order to be convolved with a two dimensional kernel. Therefore, our first step is to reduce the color image to a grayscale image, which essenitally \"flattens\" the three-dimensional matrix into two dimensions.\n",
    "\n",
    "We will now apply a function to reduce our matrix to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "\n",
    "image_gray = rgb2gray(image)\n",
    "plt.imshow(image_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the dimensionality (shape) of our grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "image_gray.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, our image is now represented as a two-dimensional matrix. It is still the same number of pixels (256 by 256), but there is no third dimension for RGB values. We are now ready to convolve our image with a kernel.\n",
    "\n",
    "For edge detection, we will convolve our image with two different kernels, and then combine the results. One kernel will identify vertical edges and the other will identify horizontal edges. These matrices approximate the derivative; a large derivative means a large change, which is likely to be an edge. The specific matrices we are using are known as the [Sobel operators](https://en.wikipedia.org/wiki/Sobel_operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sobel_x = np.array(\n",
    "    [[1, 0, -1],  \n",
    "    [2, 0, -2],  \n",
    "    [1, 0, -1]])\n",
    "sobel_y = np.array(\n",
    "    [[1, 2, 1], \n",
    "    [0, 0, 0], \n",
    "    [-1, -2, -1]])\n",
    "\n",
    "print(\"sobel_x\")\n",
    "print(\"------------------\")\n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in sobel_x]))\n",
    "print(\"------------------\\n\")\n",
    "print(\"sobel_y\")\n",
    "print(\"------------------\")\n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in sobel_y]))\n",
    "print(\"------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convolve our image with each matrix, and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gradient_x = signal.convolve2d (image_gray, sobel_x)\n",
    "gradient_y = signal.convolve2d (image_gray, sobel_y)\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow (image_gray, cmap=\"gray\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow (gradient_x, cmap=\"gray\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow (gradient_y, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine our x and y gradients to get the overall gradient of the image, and our edge detection is complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gradient = np.sqrt(np.square(gradient_x) + np.square(gradient_y))\n",
    "gradient *= 255.0 / gradient.max()\n",
    "plt.imshow (gradient, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Reverb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will again perform convolutions, but on audio instead of images. Audio can be represented as a one-dimensional matrix, which is just a list, where each value in the matrix is the value of the signal at a given time. These values range from -1 to 1.\n",
    "\n",
    "We'll start by loading in the two sound files we will be working with. The first recording, `anechoic1.wav`, is a dry (reverb-free) recording of the Overture from Mozart's *The Marriage of Figaro*. The second sound file, `Concertgebouw-m.wav`, is a recording from the Concertgebouw, a concert hall in Amsterdam, Netherlands. This recording captures the acoustics of the concert hall and will serve as our impulse response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mozart, mozart_sr = sf.read('assets/anechoic1.wav')\n",
    "impulse, impulse_sr = sf.read('assets/Concertgebouw-m.wav')\n",
    "print(\"Length of mozart recording in number of samples:\", mozart.shape, \"Sampling rate:\", mozart_sr)\n",
    "print(\"Length of impulse response in number of samples:\", impulse.shape, \"Sampling rate:\", impulse_sr)        \n",
    "\n",
    "assert mozart_sr == impulse_sr, 'sample rate must be the same in both files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the two audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Mozart recording:\")\n",
    "display(Audio(mozart, rate=mozart_sr))\n",
    "\n",
    "print(\"Impulse response:\")\n",
    "display(Audio(impulse, rate=impulse_sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's view the waveform of each sound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mozart_len = mozart.shape[0]\n",
    "plt.plot(np.linspace(0, mozart_len/mozart_sr, num=mozart_len), mozart)\n",
    "plt.title('Mozart recording waveform')\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "impulse_len = impulse.shape[0]\n",
    "plt.plot(np.linspace(0, impulse_len/impulse_sr, impulse_len), impulse)\n",
    "plt.title('Impulse response waveform')\n",
    "plt.xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now that we are familiar with our inputs, let's review how convolution with audio files works.\n",
    "\n",
    "Convolution with signals is similar to convolution with images as we did above, except now we are working in only one dimension instead of two. In this context, we can think of convolution as \"applying\" one signal to another. We are applying the reverb of the Concert Gebouw to the Mozart recording.\n",
    "\n",
    "With sound files, our domain becomes time which enables us to use the Convolution Theorem. The theorem states that convolution in the time domain is the same as complex multiplication in the frequency domain. In other words, multiplying the frequency content (spectra) of two signals is the same as performing convolution. So, \n",
    "\n",
    "$$ y(t) = x(t) * h(t) = IFFT(X(k)H(k)) $$ \n",
    "\n",
    "where $X(k)$ and $H(k)$ are frequency representations of the signals $x$ and $h$, and $y$ is our convolved signal.\n",
    "\n",
    "To apply the impulse response to the Mozart recording, we will perform the following steps that take advantage of the Convolution Theorem:\n",
    "1. Transform each signal into its frequency representation (spectrum) using the Fast Fourier Transform (FFT)\n",
    "2. Multiply the two spectra together\n",
    "3. Convert the resulting spectrum back into a time domain signal using the Inverse Fast Fourier Transform (IFFT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def next_power_of_2(x):  \n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "# the FFT is most efficient when the length of the signal is a power of two\n",
    "if mozart_len > impulse_len:\n",
    "    N = next_power_of_2(mozart_len)\n",
    "else:\n",
    "    N = next_power_of_2(impulse_len)\n",
    "\n",
    "# calculate the real part of the FFT for each signal \n",
    "mozart_fft = np.fft.rfft(mozart, N)\n",
    "impulse_fft = np.fft.rfft(impulse, N)\n",
    "\n",
    "# multiply the two signals\n",
    "convolved = mozart_fft * impulse_fft\n",
    "\n",
    "# use IFFT to convert to time domain signal\n",
    "mozart_reverb = np.fft.irfft(convolved)\n",
    "\n",
    "Audio(mozart_reverb, rate=mozart_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used convolution to apply reverb to a dry recording, therefore recreating what the recording would sound like if played in the Convertgebouw. Now we will see that we can adjust the amount of reverb applied by adding back a percentage of the dry recording. Listen to the following audio files to hear the effect of changing the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def mix_reverb(dry):\n",
    "    return (1 - dry) * mozart_reverb[0:mozart_len] + mozart * dry\n",
    "\n",
    "print(\"100% reverb\")\n",
    "display(Audio(mix_reverb(0), rate=mozart_sr))\n",
    "print(\"75% reverb\")\n",
    "display(Audio(mix_reverb(0.25), rate=mozart_sr))\n",
    "print(\"50% reverb\")\n",
    "display(Audio(mix_reverb(0.5), rate=mozart_sr))\n",
    "print(\"25% reverb\")\n",
    "display(Audio(mix_reverb(0.75), rate=mozart_sr))\n",
    "print(\"No reverb\")\n",
    "display(Audio(mix_reverb(1), rate=mozart_sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have applied reverb to the dry Mozart recording, can we perform the inverse operation? That is, given a dry (reverb-free) and wet (reverb) version of the same sound, can we retrieve the impulse response that was applied to the dry signal to create the wet signal? This is possible through the process of deconvolution.\n",
    "\n",
    "### Wiener Deconvolution\n",
    "\n",
    "Recall the previous equation we used when performing covolution:\n",
    "\n",
    "$$ y(t) = x(t)*h(t) = X(k)H(k) $$\n",
    "\n",
    "where $X$ and $H$ are the Fourier transforms of the respective time signals $x$ and $h$.\n",
    "\n",
    "We were able to apply reverb to the Mozart recording $x$ using an impulse response $h$ to create a version of the recording with reverb, $y$.\n",
    "\n",
    "Now, let's pretend we no longer have access to $h$. We have $x$ and $y$ and would like to calculate $h$\n",
    "\n",
    "Now, it'spossible to calculate $h$ by performing a complex **division in frequency domain:**\n",
    "\n",
    "$$ H(k) = \\dfrac{Y(k)}{X(k)} $$\n",
    "\n",
    "However, this method is impractical due to extreme senstivity to sounds, so we will be using a better way to retrieve $h$, called **Wiener Deconvolution**:\n",
    "\n",
    "\n",
    "$$ W(k) = \\dfrac{Y(k)\\overline{X(k)}}{|X(k)|^2 + \\sigma^2} $$\n",
    "\n",
    "where $\\overline{X(k)}$ is the conjugate of $X(k)$, and $\\sigma = \\lambda \\max(|X(k)|)$, where $\\lambda$ is a scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def next_power_of_2(x):  \n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "y = mozart_reverb\n",
    "x, srx = sf.read('assets/anechoic1.wav')\n",
    "\n",
    "\n",
    "scale = 3\n",
    "\n",
    "N = next_power_of_2(y.shape[0])   \n",
    "ft_y = np.fft.fft (y, N)\n",
    "ft_x = np.fft.fft (x, N)\n",
    "v =  max(abs(ft_x)) * 0.0001\n",
    "\n",
    "# resynthesis\n",
    "ir_rebuild = np.fft.irfft(ft_y * np.conj (ft_x) / (v + abs (ft_x) ** 2))\n",
    "\n",
    "sig_len = y.shape[0] - x.shape[0]\n",
    "ir_rebuild = ir_rebuild[1:sig_len] * scale\n",
    "\n",
    "Audio(ir_rebuild, rate=impulse_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's not perfect, we were able to recreate the impulse response of the Concert Gebouw with suprising accuracy. Take a listen to the original impulse and the impulse we recreated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Original impulse:\")\n",
    "display(Audio(impulse, rate=impulse_sr))\n",
    "\n",
    "print(\"Recreated impulse:\")\n",
    "display(Audio(ir_rebuild, rate=impulse_sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}